# Performance Analysis Usage Guide

This guide explains how to use the performance analysis tools to evaluate the PDF-DOCX to JSON pipeline.

## Quick Start

After running the pipeline and generating JSONs, simply run:

```bash
python3 analyze_performance.py
```

This will generate a comprehensive `performance_analysis.csv` file with all metrics.

## What Gets Analyzed

The analysis script examines:

1. **Stats Files** (`JSONs/*.stats.json`) - Generated by the pipeline for each document
2. **Dictionary** (`dental_form_dictionary.json`) - To compare available vs extracted fields
3. **JSON Outputs** (`JSONs/*.modento.json`) - For additional validation

## Output Files

### 1. performance_analysis.csv

A comprehensive spreadsheet containing:

#### Dictionary Comparison Section
- Total available fields in dictionary
- Total fields extracted from documents
- Match rate and coverage metrics

#### Overall Statistics Section
- Total documents processed
- Average accuracy across all documents
- Average fields and sections per document
- Processing performance metrics

#### Accuracy Distribution Section
- Breakdown by accuracy range (90-100%, 80-90%, etc.)
- Count and percentage for each range

#### Detailed Per-Document Metrics
- All metrics for each individual document
- Sorted by accuracy (best to worst)

#### Top/Bottom Performers
- 5 best performing documents
- 5 worst performing documents

#### Field Type Analysis
- Breakdown of field types (input, signature, date, etc.)
- Count and percentage for each type

#### Section Analysis
- Distribution of sections across all documents
- Count and percentage for each section

### 2. PERFORMANCE_ANALYSIS_SUMMARY.md

A human-readable markdown document with:
- Executive summary
- Key findings
- Recommendations
- Detailed analysis and interpretation

## How to Interpret Results

### Match Rate vs Accuracy

**Match Rate** = (Fields Matched to Dictionary / Total Fields) × 100
- Measures how many extracted fields match the existing dictionary
- **63.64%** in current run means most fields are recognized

**Accuracy** = Average match rate across all documents
- Measures consistency across documents
- **70.95%** in current run indicates good overall performance

### Dictionary Coverage

**Coverage** = (Unique Keys Used / Total Dictionary Fields) × 100
- Shows what percentage of the dictionary is being used
- **54.34%** in current run suggests we're using about half the available templates

### Performance Indicators

**Good Indicators:**
- ✅ Match rate above 70%
- ✅ Accuracy above 75%
- ✅ Low number of empty extractions
- ✅ High percentage of 90-100% accuracy documents

**Needs Improvement:**
- ⚠️ Match rate below 60%
- ⚠️ Accuracy below 60%
- ⚠️ Many documents with empty extractions
- ⚠️ High percentage of sub-50% accuracy documents

## Running Custom Analysis

You can modify `analyze_performance.py` to add custom metrics:

### Example: Count Specific Field Types

```python
# Add to analyze_stats_file function
signature_count = sum(1 for item in stats.get('matches', []) 
                     if 'signature' in item.get('matched_key', ''))
metrics['signature_fields'] = signature_count
```

### Example: Analyze Specific Sections

```python
# Add to calculate_overall_stats function
consent_fields = sum(1 for m in all_metrics 
                    for section in m.get('sections', '').split(', ')
                    if 'Consent' in section)
overall_stats['consent_field_count'] = consent_fields
```

## Troubleshooting

### No stats files found

**Problem:** Script reports "No stats files found"
**Solution:** Run the pipeline first with `python3 run_all.py`

### Empty extractions

**Problem:** Many documents show 0 fields extracted
**Solution:** 
1. Install OCR dependencies: `apt-get install tesseract-ocr poppler-utils`
2. Re-run pipeline with `python3 run_all.py`

### Low accuracy

**Problem:** Overall accuracy below 50%
**Solution:**
1. Check if dictionary matches your form types
2. Review unmatched fields in stats files
3. Add common patterns to dictionary
4. Verify text extraction quality in `output/` files

### Missing dictionary comparison

**Problem:** CSV doesn't show dictionary comparison
**Solution:** Ensure `dental_form_dictionary.json` exists in the same directory

## Advanced Usage

### Analyze Specific Documents

```python
from analyze_performance import analyze_stats_file

# Analyze a single document
metrics = analyze_stats_file('JSONs/document.modento.stats.json')
print(f"Accuracy: {metrics['match_accuracy']:.2f}%")
```

### Compare Multiple Runs

```bash
# Run 1
python3 run_all.py
python3 analyze_performance.py
mv performance_analysis.csv run1_analysis.csv

# Make changes to pipeline...

# Run 2
rm -rf output JSONs
python3 run_all.py
python3 analyze_performance.py
mv performance_analysis.csv run2_analysis.csv

# Compare results
diff run1_analysis.csv run2_analysis.csv
```

### Export to Different Formats

```python
import pandas as pd

# Read CSV
df = pd.read_csv('performance_analysis.csv')

# Export to Excel
df.to_excel('performance_analysis.xlsx', index=False)

# Export to JSON
df.to_json('performance_analysis.json', orient='records')
```

## Automation

### Run Analysis After Every Pipeline Execution

Add to the end of `run_all.py`:

```python
print("[3/3] Running performance analysis...")
subprocess.run(
    [sys.executable, str(script_dir / "analyze_performance.py")],
    cwd=script_dir,
    check=True,
)
```

### Scheduled Analysis

```bash
# Add to crontab for daily analysis
0 2 * * * cd /path/to/repo && python3 analyze_performance.py
```

## Metrics Glossary

| Metric | Description | Good Value |
|--------|-------------|------------|
| **Total Documents** | Number of successfully processed documents | ≥ 90% of input |
| **Total Fields** | Number of fields extracted from all documents | Varies by forms |
| **Match Rate** | % of fields matching dictionary | ≥ 70% |
| **Accuracy** | Average match rate per document | ≥ 75% |
| **Dictionary Coverage** | % of dictionary being used | 40-60% typical |
| **Avg Fields/Doc** | Average number of fields per document | 5-15 typical |
| **Avg Sections/Doc** | Average sections per document | 2-4 typical |

## Support

For issues or questions:
1. Check `pipeline_run.log` for execution errors
2. Review individual `.stats.json` files for specific documents
3. Examine `output/*.txt` files to verify text extraction
4. Consult `PERFORMANCE_ANALYSIS_SUMMARY.md` for recommendations

---

**Last Updated:** 2025-11-02  
**Version:** 1.0
